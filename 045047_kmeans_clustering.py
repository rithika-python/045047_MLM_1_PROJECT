# -*- coding: utf-8 -*-
"""045047_Kmeans_Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19s3pCMx2vdbCXOdurBgf5ysH-XWo4oKf

# **COVID 19 Cases Segmentation through K-Means Clustering**

1. Project Objectives | Problem Statements

```
1.1. PO1 | PS1: Identifying High-Risk Regions using Unsupervised Machine Learning Clustering Algorithms

1.2. PO2 | PS2: Identification of Appropriate Number of Segments or Clusters

1.3. PO3 | PS3: Determination of Segment or Cluster Characteristics
```

**2. Description of Data**

2.1. Data Source, Size, Shape

	2.1.1. Data Source (https://www.kaggle.com/datasets/hosammhmdali/covid-19-dataset)

	2.1.2. Data Size (9.03 MB)

	2.1.3. Data Shape (Dimension: Number of Variables-16 | Number of Records-100000 rows Ã— 16 columns)

  **2.2. Description of Variables**

**2.2.1. Index Variable(s):**

The dataset does not explicitly specify index variables. The index is automatically generated by pandas when reading the dataset.


**2.2.2. Variables or Features having Categories | Categorical Variables or Features (CV)**

1. iso_code: Country ISO code
2. continent: Continent name
3. location: Country name
4. date: Date of observation

**2.2.2.1. Variables or Features having Nominal Categories | Categorical Variables or Features - Nominal Type**

1. iso_code (CNV1)
2. continent (CNV2)
3. location (CNV3)

**2.2.2.2. Variables or Features having Ordinal Categories | Categorical Variables or Features - Ordinal Type**

None in the dataset

**2.2.3. Non-Categorical Variables or Features**

1. total_cases: Total reported COVID-19 cases
2. new_cases: New reported COVID-19 cases
3. new_cases_smoothed: Smoothed new reported COVID-19 cases
4. total_deaths: Total reported COVID-19 deaths
5. new_deaths: New reported COVID-19 deaths
6. new_deaths_smoothed: Smoothed new reported COVID-19 deaths
7. total_cases_per_million: Total reported COVID-19 cases per million people
8. new_cases_per_million: New reported COVID-19 cases per million people
9. new_cases_smoothed_per_million: Smoothed new reported COVID-19 cases per million people
10. total_deaths_per_million: Total reported COVID-19 deaths per million people
11. new_deaths_per_million: New reported COVID-19 deaths per million people
12. new_deaths_smoothed_per_million: Smoothed new reported COVID-19 deaths per million people

### **2.3. Descriptive Statistics**

#### 2.3.1. Descriptive Statistics: Categorical Variables or Features
##### 2.3.1.1. Count | Frequency Statistics
**Count of Unique Values in Categorical Features:**

- iso_code Count: 74
- continent Count: 6
- location Count: 74
- date Count: 1367

##### 2.3.1.2. Proportion (Relative Frequency) Statistics
- **Proportion of Unique Values in Categorical Features:**

- For iso_code:
  - ARG: 1.37%
  - CZE: 1.37%
  - OWID_EUN: 1.37%
  - OWID_EUR: 1.37%
  - BGR: 1.37%
... (74 unique ISO codes)

- For continent:
  - Africa: 28.86%
  - North America: 24.53%
  - Europe: 20.11%
  - Asia: 12.99%
  - South America: 9.19%
  - Oceania: 4.33%

- For location:
  - Argentina: 1.37%
  - Czechia: 1.37%
  - European Union: 1.37%
  - Europe: 1.37%
  - Bulgaria: 1.37%
... (74 unique locations)

- For date:
  - 24-03-2021: 0.07%
  - 29-11-2020: 0.07%
  - 12-12-2020: 0.07%
  - 11-12-2020: 0.07%
  - 10-12-2020: 0.07%
... (1367 unique dates)

#### **2.3.2. Descriptive Statistics: Non-Categorical Variables or Features**
##### **2.3.2.1. Measures of Central Tendency**
 - Total Cases:
  - Mean: 5.94 million
  - Median: 66,218

- New Cases:
  - Mean: 9,726
  - Median: 5

- New Cases Smoothed:
  - Mean: 9,762
  - Median: 29.43

- Total Deaths:
  - Mean: 63,644
  - Median: 1,168

- New Deaths:
  - Mean: 69.45
  - Median: 0

- New Deaths Smoothed:
  - Mean: 69.70
  - Median: 0.14

- Total Cases Per Million:
  - Mean: 96,903
  - Median: 19,343

- New Cases Per Million:
  - Mean: 142.45
  - Median: 0.50

- New Cases Smoothed Per Million:
  - Mean: 142.99
  - Median: 6.86

- Total Deaths Per Million:
  - Mean: 852.31
  - Median: 324.96

- New Deaths Per Million:
  - Mean: 0.95
  - Median: 0

- New Deaths Smoothed Per Million:
  - Mean: 0.96
  - Median: 0.02

##### **2.3.2.2. Measures of Dispersion**
- **Standard Deviation:**
  - Total Cases: 30,148,910
  - New Cases: 122,643
  - New Cases Smoothed: 101,261
  - Total Deaths: 256,746
  - New Deaths: 485.51
  - New Deaths Smoothed: 379.70
  - Total Cases Per Million: 151,270.3
  - New Cases Per Million: 800.10
  - New Cases Smoothed Per Million: 514.07
  - Total Deaths Per Million: 1,102.21
  - New Deaths Per Million: 5.24
  - New Deaths Smoothed Per Million: 2.76

##### **2.3.2.3. Correlation Statistics (with Test of Correlation)**
- **Correlation Matrix:**
1. total_cases vs. new_cases: 0.266262
2. total_cases vs. total_deaths: 0.921228
3. new_cases vs. new_deaths: 0.430335

###**ANALYSIS OF DATA**###
### **3.1. Data Pre-Processing**

#### **3.1.1. Missing Data Statistics and Treatment**
- count    99999.000000
- mean         3.428972
- std         10.658129
- min          0.000000
- 25%          0.000000
- 50%          0.000000
- 75%          0.000000
- max         81.250000

#### 3.1.2. Numerical Encoding of Categorical Variables or Features
- **Ordinal Encoder:**
   - Used to encode categorical variables into numerical format.

	 #### 3.1.3. Outlier Statistics and Treatment
- **Outlier Detection:**
   - Identified outliers using boxplot visualization.
  
- **Outlier Treatment:**
   - Outliers treated by performing normalization using Min-Max Scaler.

#### 3.1.4. Data Transformation and Scaling
- **Normalization:**
   - Min-Max Scaling applied to non-categorical variables to bring them within a common scale.

#### 3.1.5. Pre-Processed Dataset
- **Pre-Processed Categorical Data Subset:**
   - Encoded categorical data using Ordinal Encoder.
  
- **Pre-Processed Non-Categorical Data Subset:**
   - Normalized non-categorical data using Min-Max Scaler.

- **Pre-Processed Dataset:**
   - Merged pre-processed categorical and non-categorical data subsets.

# **3.2. Data Analysis**

In this section, we conduct a comprehensive analysis of the data using the K-Means clustering algorithm and evaluate its performance using various metrics.

#### **3.2.1. Unsupervised Machine Learning Clustering Algorithm: K-Means (Base Model) | Metrics Used - Euclidean Distance**

K-Means is employed to partition data points into K clusters based on the Euclidean distance metric. The choice of K significantly influences the granularity of the clustering and the interpretability of the results.

#### **3.2.2. Clustering Model Performance Evaluation**

##### **3.2.2.1. K Means Clustering Using the Elbow Method**

In the Elbow method, we vary the number of clusters (K) from 1 to 10 and calculate WCSS (Within-Cluster Sum of Square) for each value of K. WCSS is the sum of the squared distance between each point and the centroid in a cluster. When plotted against the number of clusters, the plot exhibits an "Elbow" shape. The optimal value of K is identified at the point where the plot shows a significant change in slope before becoming almost parallel to the X-axis.

##### **3.2.2.2. Silhouette Score | Davies-Bouldin Score (Base Model: K-Means)**

- **Silhouette Score**: Measures the cohesion and separation of clusters, with values ranging from -1 to 1. Higher scores indicate better-defined clusters.
- **Davies-Bouldin Index**: Measures the average similarity between each cluster and its most similar cluster. Lower values suggest better clustering.

The evaluation is performed for different cluster sizes to identify the optimal number of clusters that maximize the Silhouette Score and minimize the Davies-Bouldin Index.

##### **3.2.2.3. Clustering Model Performance Evaluation: Time Statistics | (CPU | GPU) Memory Statistics (Base Model: K-Means)**

- **Time Statistics**: Includes the time taken by the K-Means algorithm to converge for a given number of clusters.
- **Memory Statistics**: Refers to the memory consumption (CPU or GPU) by the K-Means algorithm during execution.

These statistics offer insights into the computational efficiency and scalability of K-Means for different cluster sizes.

# **Detailed Analysis**

#### K Means Clustering Using the Elbow Method:

- The Elbow method is employed to determine the optimal number of clusters (K) by analyzing the plot of WCSS against the number of clusters.
- The "Elbow" point on the plot indicates a significant change in slope, suggesting the optimal value of K.

#### Silhouette Score and Davies-Bouldin Index Analysis:
```
- **For 2 clusters**:
  - Silhouette Score: 0.6166100594490603
  - Davies-Bouldin Index: 0.5136045731337742
- **For 3 clusters**:
  - Silhouette Score: 0.5702211048517893
  - Davies-Bouldin Index: 0.5269052484537392
- **For 4 clusters** (Chosen Optimal):
  - Silhouette Score: 0.5405505117855898
  - Davies-Bouldin Index: 0.5432884695784986
- **For 5 clusters**:
  - Silhouette Score: 0.5171916374109905
  - Davies-Bouldin Index: 0.5622377995508524
```
These metrics aid in determining the optimal number of clusters, balancing cohesion and separation.

#### **Time and Memory Statistics Analysis:**
```
  - Time taken by K-Means for k=3: 1.2459 seconds
  - Memory used by K-Means for k=3: 1653.42 MB

This analysis showcases the computational efficiency and resource requirements of K-Means, guiding the selection of an appropriate cluster size.
```

### **Conclusion**

The evaluation of the K-Means clustering model provides valuable insights into the clustering quality and computational efficiency. Based on the metrics analyzed, K=3 is chosen as the optimal number of clusters, ensuring meaningful groupings of data points while maintaining computational scalability. Theoretical insights into the metrics used enhance the understanding of the clustering process and its outcomes, facilitating informed decision-making. Additionally, the Elbow method aids in confirming the optimal value of K, providing a comprehensive approach to cluster analysis.

#**3.2.3.1. PO3 | PS3:: Cluster Analysis: Base Model (K-Means)**

```
Based on the centroid values and characteristics, we can assign the following names to the clusters:

Cluster 1: Mixed Characteristics
This cluster has centroid values indicating a mixture of low and moderate values for both new deaths and smoothed new deaths.

Cluster 2: Low New Deaths, High Smoothed New Deaths
This cluster has centroid values indicating low values for new deaths but relatively high values for smoothed new deaths.

Cluster 3: Mixed Characteristics
This cluster also exhibits mixed characteristics similar to Cluster 1, with a mixture of low and moderate values for both new deaths and smoothed new deaths.
```

These names provide a general categorization based on the characteristics of the clusters and can help in interpreting the clusters' attributes. Adjustments to the names may be necessary based on the specific characteristics observed in the dataset.

**3.2.3.1.1. Cluster Analysis with Categorical Variables or Features: Chi-Square Test of Independence**

```
Chi-Square Test of Independence for variable 'iso_code':
   - Chi-Square Value: 47526.42797721641
   - P-value: 0.0
Chi-Square Test of Independence for variable 'continent_code':
   - Chi-Square Value: 3806.130983735541
   - P-value: 0.0
Chi-Square Test of Independence for variable 'location_code':
   - Chi-Square Value: 47526.42797721642
   - P-value: 0.0
Chi-Square Test of Independence for variable 'date_code':
   - Chi-Square Value: 2834.8299940206184
   - P-value: 0.08332773397857907
Chi-Square Test of Independence for variable 'cluster':
   - Chi-Square Value: 199998.0
   - P-value: 0.0
```
The Chi-Square Test of Independence is a statistical test used to determine whether there is a significant association between two categorical variables. In this context, the test is applied to assess the relationship between each categorical variable (e.g., 'iso_code', 'continent_code', 'location_code', 'date_code', and 'cluster') and other variables in the dataset.

The results of the Chi-Square Test of Independence are interpreted as follows:

1. **iso_code:** The Chi-Square value is very high (47526.43), indicating a strong association between iso_code and cluster labels.
The p-value is 0.0, indicating that this association is statistically significant.

2. **continent_code:** Similar to iso_code, there is a strong association between continent_code and cluster labels, with a high Chi-Square value (3806.13) and a p-value of 0.0.

3. **location_code:** Again, there is a strong association between location_code and cluster labels, as indicated by the high Chi-Square value (47526.43) and a p-value of 0.0.

4. **date_code:** The Chi-Square value is moderate (2834.83), suggesting a moderate association between date_code and cluster labels. However, the p-value (0.083) is greater than the typical significance level of 0.05, indicating that this association may not be statistically significant at the conventional level.

5. **cluster:** There is a very strong association between the cluster labels and themselves (which is expected), resulting in a very high Chi-Square value (199998.0) and a p-value of 0.0.

Overall, the Chi-Square Test of Independence indicates significant associations between the categorical variables (iso_code, continent_code, location_code, and cluster) and the cluster labels, except for date_code, where the association may not be statistically significant.

**3.2.3.1.2. Cluster Analysis with Non-Categorical Variables or Features: Analysis of Variance (ANOVA)**

The ANOVA results indicate whether there are statistically significant differences in the means of the features across different clusters. Here's what each part of the ANOVA results means:

1. total_cases_mmnorm, new_cases_mmnorm, new_cases_smoothed_mmnorm, total_deaths_mmnorm, new_deaths_mmnorm, new_deaths_smoothed_mmnorm:

  - The ANOVA results indicate statistically significant differences in the means of these features across the clusters. This implies that the clusters exhibit varying levels of total cases, new cases, new cases smoothed, total deaths, new deaths, and new deaths smoothed.

2. total_cases_per_million_mmnorm, new_cases_per_million_mmnorm, new_cases_smoothed_per_million_mmnorm, total_deaths_per_million_mmnorm, new_deaths_per_million_mmnorm, new_deaths_smoothed_per_million_mmnorm:

  - Similar to the previous features, there are statistically significant differences in the means across clusters for these per million features. This suggests varying rates of total cases, new cases, new cases smoothed, total deaths, new deaths, and new deaths smoothed per million people among the clusters.

In summary, the ANOVA results indicate that there are statistically significant differences in the means of the features across different clusters. This suggests that the clustering algorithm has successfully partitioned the data into clusters that exhibit distinct characteristics in terms of selling prices, kilometers driven, and their normalized versions.

#**Managerial Insights:**
1. Data Preprocessing Efficiency:

  - The code efficiently handles missing data through imputation techniques, ensuring that the dataset is complete and suitable for analysis.
  - Categorical variables are appropriately encoded to numeric values, facilitating further analysis.

2. Clustering Analysis for Understanding Patterns:

  - Clustering techniques such as K-Means are utilized to identify inherent patterns within the COVID-19 data.
  - Silhouette Score and Davies-Bouldin Index are employed to evaluate the quality of clustering, providing insights into the effectiveness of the clustering algorithms.

3. Interpretation of Clusters:

  - The code provides interpretation of clusters based on centroid values, allowing for the characterization of clusters with meaningful labels.
  - This allows managers to understand the distinct characteristics of each cluster, aiding in decision-making and resource allocation strategies.

4. Statistical Analysis for Further Understanding:

  - Chi-Square Test of Independence and ANOVA tests are conducted to analyze the relationship between clusters and categorical/non-categorical features.
  - These statistical tests provide insights into the significance of variables in defining cluster characteristics and understanding differences across clusters.

5. Time and Memory Efficiency:

  - Time and memory statistics are provided, offering insights into the computational resources required for the analysis.
  - This information can guide decisions on the scalability of the analysis and the allocation of computing resources.

Overall Decision:

The managerial insights derived from the code and operations allow for informed decision-making in various aspects such as public health policy formulation, resource allocation, and targeted interventions.
Managers can use these insights to tailor strategies based on the specific characteristics and needs of different clusters of COVID-19 cases, ultimately contributing to more effective pandemic management and control efforts.
"""

# Required Libraries
import pandas as pd, numpy as np # For Data Manipulation
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For Encoding Categorical Data [Nominal | Ordinal]
from sklearn.preprocessing import OneHotEncoder # For Creating Dummy Variables of Categorical Data [Nominal]
from sklearn.impute import SimpleImputer, KNNImputer # For Imputation of Missing Data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # For Rescaling Data
from sklearn.model_selection import train_test_split # For Splitting Data into Training & Testing Sets
import pandas as pd, numpy as np # For Data Manipulation
import matplotlib.pyplot as plt, seaborn as sns # For Data Visualization
import scipy.cluster.hierarchy as sch # For Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering as agclus, KMeans as kmclus # For Agglomerative & K-Means Clustering
from sklearn.metrics import silhouette_score as sscore, davies_bouldin_score as dbscore # For Clustering Model Evaluation

df=pd.read_csv("/content/mlm_dataset.csv")
df

df_cat = df[['iso_code', 'continent', 'location', 'date']] # Categorical Data [Nominal | Ordinal]
df_noncat = df[['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million', 'total_deaths_per_million', 'new_deaths_per_million', 'new_deaths_smoothed_per_million']] # Non-Categorical Data

print(df.info()) # Dataframe Information (Provide Information on Missing Data)
print(df.describe())
variable_missing_data = df.isna().sum(); variable_missing_data # Variable-wise Missing Data Information
print(variable_missing_data)
record_missing_data = df.isna().sum(axis=1).sort_values(ascending=False).head(5); record_missing_data # Record-wise Missing Data Information (Top 5)
print(record_missing_data)

# 1.2.1. Impute Missing Categorical Data [Nominal | Ordinal] using Descriptive Satatistics : Central Tendency (Mode)


# Dataset Used : df_cat

si_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # Strategy = median [When Odd Number of Categories Exists]
si_cat_fit = si_cat.fit_transform(df_cat)
df_cat_mdi = pd.DataFrame(si_cat_fit, columns=df_cat.columns); df_cat_mdi # Missing Categorical Data Imputed Subset
df_cat_mdi.info()

# 1.2.2.1. Impute Missing Non-Categorical Data using Descriptive Statistics : Central Tendency
# --------------------------------------------------------------------------------------------

# Dataset Used : df_noncat

si_noncat = SimpleImputer(missing_values=np.nan, strategy='mean') # Other Strategy : mean | median | most_frequent | constant
si_noncat_fit = si_noncat.fit_transform(df_noncat)
df_noncat_mdi_si = pd.DataFrame(si_noncat_fit, columns=df_noncat.columns); df_noncat_mdi_si # Missing Non-Categorical Data Imputed Subset using Simple Imputer
df_noncat_mdi_si.info()

#Numeric Encoding of Categorical Data [Nominal & Ordinal]
df_cat_mdt_code = df_cat_mdi.copy()
oe = OrdinalEncoder()
oe_fit = oe.fit_transform(df_cat_mdt_code)
df_cat_code_oe = pd.DataFrame(oe_fit, columns=['iso_code_code', 'continent_code', 'location_code', 'date_code'])
df_cat_mdt_code_oe = df_cat_mdt_code.join(df_cat_code_oe); df_cat_mdt_code_oe # (Missing Data Treated) Numeric Coded Categorical Dataset using Scikit Learn Ordinal Encoder
df_cat_mdt_code_oe = pd.merge(df_cat_mdt_code, df_cat_code_oe, left_index=True, right_index=True);
df_cat_mdt_code_oe

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming df is your DataFrame
df_noncat = df[['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million', 'total_deaths_per_million', 'new_deaths_per_million', 'new_deaths_smoothed_per_million']] # Non-Categorical Data

# Create vertical boxplot for 'total_cases'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['total_cases'])
plt.title('Boxplot of total_cases')
plt.ylabel('total_cases')
plt.show()

# Create vertical boxplot for 'new_cases'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_cases'])
plt.title('Boxplot of new_cases')
plt.ylabel('new_cases')
plt.show()

# Create vertical boxplot for 'new_cases_smoothed'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_cases_smoothed'])
plt.title('Boxplot of new_cases_smoothed')
plt.ylabel('new_cases_smoothed')
plt.show()

# Create vertical boxplot for 'total_deaths'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['total_deaths'])
plt.title('Boxplot of total_deaths')
plt.ylabel('total_deaths')
plt.show()

# Create vertical boxplot for 'new_deaths'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_deaths'])
plt.title('Boxplot of new_deaths')
plt.ylabel('new_deaths')
plt.show()

# Create vertical boxplot for 'new_deaths_smoothed'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_deaths_smoothed'])
plt.title('Boxplot of new_deaths_smoothed')
plt.ylabel('new_deaths_smoothed')
plt.show()

# Create vertical boxplot for 'total_cases_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['total_cases_per_million'])
plt.title('Boxplot of total_cases_per_million')
plt.ylabel('total_cases_per_million')
plt.show()

# Create vertical boxplot for 'new_cases_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_cases_per_million'])
plt.title('Boxplot of new_cases_per_million')
plt.ylabel('new_cases_per_million')
plt.show()

# Create vertical boxplot for 'new_cases_smoothed_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_cases_smoothed_per_million'])
plt.title('Boxplot of new_cases_smoothed_per_million')
plt.ylabel('new_cases_smoothed_per_million')
plt.show()

# Create vertical boxplot for 'total_deaths_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['total_deaths_per_million'])
plt.title('Boxplot of total_deaths_per_million')
plt.ylabel('total_deaths_per_million')
plt.show()

# Create vertical boxplot for 'new_deaths_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_deaths_per_million'])
plt.title('Boxplot of new_deaths_per_million')
plt.ylabel('new_deaths_per_million')
plt.show()

# Create vertical boxplot for 'new_deaths_smoothed_per_million'
plt.figure(figsize=(8, 6))
sns.boxplot(y=df_noncat['new_deaths_smoothed_per_million'])
plt.title('Boxplot of new_deaths_smoothed_per_million')
plt.ylabel('new_deaths_smoothed_per_million')
plt.show()

# 3.2.1. Normalization : Min-Max Scaling
mms = MinMaxScaler()
mms_fit = mms.fit_transform(df_noncat_mdi_si [['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million', 'total_deaths_per_million', 'new_deaths_per_million', 'new_deaths_smoothed_per_million']])
df_noncat_minmax_norm = pd.DataFrame(mms_fit, columns=['total_cases_mmnorm', 'new_cases_mmnorm', 'new_cases_smoothed_mmnorm', 'total_deaths_mmnorm', 'new_deaths_mmnorm', 'new_deaths_smoothed_mmnorm', 'total_cases_per_million_mmnorm', 'new_cases_per_million_mmnorm', 'new_cases_smoothed_per_million_mmnorm', 'total_deaths_per_million_mmnorm', 'new_deaths_per_million_mmnorm', 'new_deaths_smoothed_per_million_mmnorm']); df_noncat_minmax_norm

#df_noncat_minmax_norm = pd.DataFrame(mms_fit, columns=df_noncat_mdt.columns+'_mmnorm'); df_noncat_minmax_norm
#df_noncat_mdt_mmn = df_noncat_mdi_si.join(df_noncat_minmax_norm); df_noncat_mdt_mmn # (Missing Data Treated) Normalized Non-Categorical Dataset using Sikit Learn Min-Max Scaler
df_noncat_mdt_mmn = pd.merge(df_noncat_mdi_si, df_noncat_minmax_norm, left_index=True, right_index=True); df_noncat_mdt_mmn

# Pre-Processed Dataset

# Using Scikit Learn Ordinal Encoder : df_cat_mdt_code_oe [Superior]

# Missing Data Treated & Transformed or Rescaled Non-Categorical Data Subsets

# Using Scikit Learn Standard Scaler : df_noncat_mdt_std [Standardization]


# Pre-Processed Categorical Data Subset
df_cat_ppd = df_cat_mdt_code_oe.copy(); df_cat_ppd # Preferred Data Subset

# Pre-Processed Non-Categorical Data Subset
df_noncat_ppd = df_noncat_mdt_mmn.copy(); df_noncat_ppd


# Pre-Processed Dataset
df_ppd = df_cat_ppd.join(df_noncat_ppd); df_ppd # Pre-Processed Dataset
df_ppd = pd.merge(df_cat_ppd, df_noncat_ppd, left_index=True, right_index=True); df_ppd

#K-Means Clustering


# 2.1.1. Determine Value of 'K' in K-Means using Elbow Curve & KMeans-Inertia

'''
KMeans-Inertia : Sum of Squared Distances of Samples to their closest Cluster Center (Centroid), Weighted by the Sample Weights (if provided)
'''
train_new_df=df_ppd[['total_cases_mmnorm', 'new_cases_mmnorm', 'new_cases_smoothed_mmnorm', 'total_deaths_mmnorm', 'new_deaths_mmnorm', 'new_deaths_smoothed_mmnorm', 'total_cases_per_million_mmnorm', 'new_cases_per_million_mmnorm', 'new_cases_smoothed_per_million_mmnorm', 'total_deaths_per_million_mmnorm', 'new_deaths_per_million_mmnorm', 'new_deaths_smoothed_per_million_mmnorm','iso_code_code', 'continent_code', 'location_code', 'date_code']]
wcssd = [] # Within-Cluster-Sum-Squared-Distance
nr_clus = range(1,11) # Number of Clusters
for k in nr_clus:
    kmeans = kmclus(n_clusters=k, init='random', random_state=111)
    kmeans.fit(train_new_df)
    wcssd.append(kmeans.inertia_)
plt.plot(nr_clus, wcssd, marker='x')
plt.xlabel('Values of K')
plt.ylabel('Within Cluster Sum Squared Distance')
plt.title('Elbow Curve for Optimal K')
print(plt.show())


cluster_range = [2, 3, 4, 5]

# Iterate over different numbers of clusters
for k in cluster_range:
  # Perform Kmeans Clustering
    km_cluster = kmclus(n_clusters=k, init='random', random_state=222)
    print(km_cluster)
    km_cluster_model = km_cluster.fit_predict(train_new_df);
    print(km_cluster_model)
# Calculate Silhouette Score
    sscore_km_cluster = sscore(train_new_df, km_cluster_model);
    print(f"Silhouette Score for {k} clusters: {sscore_km_cluster}")
    # Calculate Davies-Bouldin Index
    dbscore_km_cluster = dbscore(train_new_df, km_cluster_model)
    print(f"Davies-Bouldin Index for {k} clusters: {dbscore_km_cluster}\n")

import time
import psutil
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns

# Start time
start_time = time.time()

# Specify the number of clusters
num_clusters = 3

# Initialize KMeans object
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Fit KMeans clustering model to the data
kmeans.fit(train_new_df[['new_deaths_mmnorm', 'new_deaths_smoothed_mmnorm']])

# End time
end_time = time.time()

# Total time taken
time_taken = end_time - start_time
print("Time taken by K-Means for k=4:", "{:.4f} seconds".format(time_taken))

# Memory used by the KMeans model
memory_used = psutil.Process().memory_info().rss / 1024 / 1024  # Memory in MB
print("Memory used by K-Means for k=4:", "{:.2f} MB".format(memory_used))

# Get cluster labels
cluster_labels = kmeans.labels_

# Add cluster labels to DataFrame
train_new_df['cluster'] = cluster_labels

# Get centroids of each cluster
centroids = kmeans.cluster_centers_

# Visualize the clusters using seaborn
plt.figure(figsize=(10, 6))
sns.scatterplot(data=train_new_df, x='new_deaths_mmnorm', y='new_deaths_smoothed_mmnorm', hue='cluster', palette='viridis', s=100)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='black', label='Centroids')
plt.xlabel('New Deaths (Normalized)')
plt.ylabel('Smoothed New Deaths (Normalized)')
plt.title('K-means Clustering with Centroids')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

import pandas as pd
from scipy.stats import chi2_contingency

# Assuming train_new_df contains your DataFrame with categorical variables and cluster labels

# Define your categorical variables for analysis
categorical_variables = ['iso_code_code', 'continent_code', 'location_code', 'date_code', 'cluster']

# Create contingency table for each categorical variable and the cluster labels
contingency_tables = {}
for variable in categorical_variables:
    contingency_tables[variable] = pd.crosstab(train_new_df[variable], train_new_df['cluster'])

# Perform Chi-Square Test of Independence for each contingency table
chi2_results = {}
for variable, contingency_table in contingency_tables.items():
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    chi2_results[variable] = {'Chi-Square': chi2, 'P-value': p}

# Display the results
for variable, result in chi2_results.items():
    print(f"Chi-Square Test of Independence for variable '{variable}':")
    print(f"   - Chi-Square Value: {result['Chi-Square']}")
    print(f"   - P-value: {result['P-value']}")

from scipy.stats import f_oneway

# List of features for ANOVA
features = ['total_cases_mmnorm', 'new_cases_mmnorm', 'new_cases_smoothed_mmnorm',
            'total_deaths_mmnorm', 'new_deaths_mmnorm', 'new_deaths_smoothed_mmnorm',
            'total_cases_per_million_mmnorm', 'new_cases_per_million_mmnorm',
            'new_cases_smoothed_per_million_mmnorm', 'total_deaths_per_million_mmnorm',
            'new_deaths_per_million_mmnorm', 'new_deaths_smoothed_per_million_mmnorm']

# Initialize an empty dictionary to store ANOVA results
anova_results = {}

# Iterate over unique cluster labels
for cluster_label in train_new_df['cluster'].unique():
    # Filter dataframe for the current cluster
    cluster_df = train_new_df[train_new_df['cluster'] == cluster_label]

    # Initialize an empty dictionary to store ANOVA results for the current cluster
    cluster_anova_results = {}

    # Perform ANOVA for each feature
    for feature in features:
        # Perform ANOVA test
        anova_result = f_oneway(*[train_new_df[train_new_df['cluster'] == i][feature] for i in train_new_df['cluster'].unique()])
        # Store ANOVA result
        cluster_anova_results[feature] = anova_result

    # Store ANOVA results for the current cluster
    anova_results[cluster_label] = cluster_anova_results

# Print ANOVA results
for cluster_label, cluster_anova_result in anova_results.items():
    print(f"Cluster {cluster_label} ANOVA Results:")
    for feature, result in cluster_anova_result.items():
        print(f"{feature}: F-value = {result.statistic}, p-value = {result.pvalue}")

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df_ppd_round_kmcluster1 is your DataFrame
variables = ['total_cases_mmnorm', 'new_cases_mmnorm', 'new_cases_smoothed_mmnorm',
                          'total_deaths_mmnorm', 'new_deaths_mmnorm', 'new_deaths_smoothed_mmnorm',
                          'total_cases_per_million_mmnorm', 'new_cases_per_million_mmnorm',
                          'new_cases_smoothed_per_million_mmnorm', 'total_deaths_per_million_mmnorm',
                          'new_deaths_per_million_mmnorm', 'new_deaths_smoothed_per_million_mmnorm',
                          'cluster']

# Create a pairplot for all combinations
sns.pairplot(train_new_df[variables], hue='cluster', palette='viridis', diag_kind='kde')
plt.suptitle('Pairplot of Variables in Different Clusters', y=1.02)
plt.show()

"""***Paired Plots in Clustering Analysis***

### Introduction

In clustering analysis, understanding the relationships between variables and how they relate to the formation of clusters is crucial for interpreting the results of the clustering algorithm. Paired plots, also known as scatterplot matrices, are powerful visualization tools that facilitate this understanding by providing insights into the multivariate structure of the data.

### Theory of Paired Plots

Paired plots consist of a grid of scatterplots where each variable is plotted against every other variable in the dataset. This results in a comprehensive visualization of the relationships between pairs of variables. Along the diagonal of the grid, histograms or density plots of individual variables are displayed to show their distributions.

### Importance in Clustering

1. **Visualizing Cluster Separation**: Paired plots allow us to visualize how well clusters are separated in multivariate space. By examining the scatterplots, we can assess the degree of separation between clusters and identify any overlapping regions, which helps in evaluating the quality of the clustering algorithm.

2. **Identifying Cluster Patterns**: Paired plots help in identifying patterns within clusters. By observing the relationships between pairs of variables within the same cluster, we can gain insights into the characteristics of the clusters and understand the underlying structure of the data.

3. **Detecting Outliers**: Paired plots aid in detecting outliers within clusters. Outliers are data points that deviate significantly from the rest of the data and may indicate errors or anomalies. By visually inspecting the scatterplots, we can identify any data points that lie far away from the cluster centroids or exhibit unusual patterns.

4. **Assessing Cluster Homogeneity**: Paired plots assist in assessing the homogeneity of clusters. Homogeneous clusters contain data points that are similar to each other, while heterogeneous clusters contain data points that are dissimilar. By examining the scatterplots, we can determine whether clusters exhibit similar patterns across different pairs of variables, which provides insights into their homogeneity.

5. **Feature Selection**: Paired plots help in feature selection for clustering. By visualizing the relationships between pairs of variables, we can identify which variables are most relevant for distinguishing between clusters and which variables may be redundant or less informative.

### Conclusion

Paired plots are valuable tools in clustering analysis, providing a comprehensive visualization of the relationships between variables and their importance in the formation and interpretation of clusters. By leveraging the insights gained from paired plots, researchers and practitioners can make informed decisions about clustering algorithms, feature selection, and cluster interpretation.

"""



