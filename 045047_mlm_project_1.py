# -*- coding: utf-8 -*-
"""045047_MLM Project 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W_rIFojnuRjSe5MmwMfKNxOV_nBM8lHV
"""

# Required Libraries
import pandas as pd, numpy as np # For Data Manipulation
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder # For Encoding Categorical Data [Nominal | Ordinal]
from sklearn.preprocessing import OneHotEncoder # For Creating Dummy Variables of Categorical Data [Nominal]
from sklearn.impute import SimpleImputer, KNNImputer # For Imputation of Missing Data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler # For Rescaling Data
from sklearn.model_selection import train_test_split # For Splitting Data into Training & Testing Sets

df=pd.read_csv("mlm_project_1_dataset.csv")
df

# Data Bifurcation
df_cat = df[['Category', 'Currency', 'Minimum Android', 'Content Rating', 'Free', 'Ad Supported', 'In App Purchases', 'Editors Choice']] # Categorical Data [Nominal | Ordinal]
df_noncat = df[['Rating', 'Rating Count', 'Installs', 'Minimum Installs', 'Maximum Installs', 'Price', 'Size']] # Non-Categorical Data

# Dataset Used : df

df.info() # Dataframe Information (Provide Information on Missing Data)
variable_missing_data = df.isna().sum(); variable_missing_data # Variable-wise Missing Data Information
record_missing_data = df.isna().sum(axis=1).sort_values(ascending=False).head(5); record_missing_data # Record-wise Missing Data Information (Top 5)

# 1.2. Missing Data Treatment
# ---------------------------

# 1.2.1. Impute Missing Categorical Data [Nominal | Ordinal] using Descriptive Satatistics : Central Tendency (Mode)
# ------------------------------------------------------------------------------------------------------------------

# Dataset Used : df_cat

si_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # Strategy = median [When Odd Number of Categories Exists]
si_cat_fit = si_cat.fit_transform(df_cat)
df_cat_mdi = pd.DataFrame(si_cat_fit, columns=df_cat.columns); df_cat_mdi # Missing Categorical Data Imputed Subset
print(df_cat_mdi.info())
print(df_cat_mdi)

# Convert 'Rating' column to numeric, handle NaN
si_noncat = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # Other Strategy : mean | median | most_frequent | constant
si_noncat_fit = si_noncat.fit_transform(df_noncat)
df_noncat_mdi_si = pd.DataFrame(si_noncat_fit, columns=df_noncat.columns); df_noncat_mdi_si # Missing Non-Categorical Data Imputed Subset using Simple Imputer
print(df_noncat_mdi_si.info())
print(df_noncat_mdi_si)

# Using Scikit Learn : Ordinal Encoder (Superior)
#Numeric Encoding of Categorical Data [Nominal & Ordinal]
df_cat_mdt_code = df_cat_mdi.copy()
oe = OrdinalEncoder()
oe_fit = oe.fit_transform(df_cat_mdt_code)
df_cat_code_oe = pd.DataFrame(oe_fit, columns=['Category_code', 'Currency_code', 'Minimum Android_code', 'Content Rating_code', 'Free_code', 'Ad Supported_code', 'In App Purchases_code', 'Editors Choice_code']); df_cat_code_oe
df_cat_mdt_code_oe = df_cat_mdt_code.join(df_cat_code_oe); df_cat_mdt_code_oe # (Missing Data Treated) Numeric Coded Categorical Dataset using Scikit Learn Ordinal Encoder
df_cat_mdt_code_oe = pd.merge(df_cat_mdt_code, df_cat_code_oe, left_index=True, right_index=True);
df_cat_mdt_code_oe

#Data Transformation & Rescaling [Treatment of Outliers]

# Dataset Used : df_noncat_mdt

# Scaling Variable : income

# 3.1. Standardization
ss = StandardScaler()
ss_fit = ss.fit_transform(df_noncat_mdi_si[['Rating','Rating Count','Minimum Installs', 'Maximum Installs','Price']])
df_noncat_std = pd.DataFrame(ss_fit, columns=['Rating_std','Rating Count_std','Minimum Installs_std', 'Maximum Installs_std','Price_std']);
print(df_noncat_std)
#df_noncat_std = pd.DataFrame(ss_fit, columns=df_noncat.columns+'_std');
#print(df_noncat_std)
df_noncat_mdt_std = df_noncat.join(df_noncat_std);
print(df_noncat_mdt_std) # (Missing Data Treated) Standardized Non-Categorical Dataset using Scikit Learn Standard Scaler
df_noncat_mdt_std = pd.merge(df_noncat_mdi_si, df_noncat_std, left_index=True, right_index=True);
print(df_noncat_mdt_std)

# Pre-Processed Dataset
# ---------------------

# Missing Data Treated Subsets
# ----------------------------
# Categorical Data Subset : df_cat_mdi
# Non-Categorical Data Subset : df_noncat_mdi_si

# Missing Data Treated & Numeric Coded Categorical Data Subsets

# Using Scikit Learn Ordinal Encoder : df_cat_mdt_code_oe [Superior]

# Missing Data Treated & Transformed or Rescaled Non-Categorical Data Subsets
# ---------------------------------------------------------------------------
# Using Scikit Learn Standard Scaler : df_noncat_mdt_std [Standardization]


# Pre-Processed Categorical Data Subset
df_cat_ppd = df_cat_mdt_code_oe.copy(); df_cat_ppd # Preferred Data Subset

# Pre-Processed Non-Categorical Data Subset
df_noncat_ppd = df_noncat_mdt_std.copy(); df_noncat_ppd


# Pre-Processed Dataset
df_ppd = df_cat_ppd.join(df_noncat_ppd); df_ppd # Pre-Processed Dataset
df_ppd = pd.merge(df_cat_ppd, df_noncat_ppd, left_index=True, right_index=True);
print(df_ppd)

#Data Bifurcation [Training & Testing Datasets]

# Dataset Used : df_ppd

train_df, test_df = train_test_split(df_ppd, test_size=0.25, random_state=1234)
print(train_df) # Training Dataset
print(test_df) # Testing Dataset

from sklearn.cluster import KMeans #K-Means Clustering
import scipy.cluster.hierarchy as sch # For Hierarchical Clustering
from sklearn.cluster import AgglomerativeClustering as agclus, KMeans as kmclus # For Agglomerative & K-Means Clustering
from sklearn.metrics import silhouette_score as sscore, davies_bouldin_score as dbscore # For Clustering Model Evaluation
import matplotlib.pyplot as plt

# 2.1.1. Determine Value of 'K' in K-Means using Elbow Curve & KMeans-Inertia

'''
KMeans-Inertia : Sum of Squared Distances of Samples to their closest Cluster Center (Centroid), Weighted by the Sample Weights (if provided)
'''
train_new_df=train_df[['Rating','Price','Minimum Installs','Category_code', 'Currency_code', 'Minimum Android_code', 'Content Rating_code', 'Free_code', 'Ad Supported_code', 'In App Purchases_code','Editors Choice_code']]
wcssd = [] # Within-Cluster-Sum-Squared-Distance
nr_clus = range(1,11) # Number of Clusters
for k in nr_clus:
    kmeans = KMeans(n_clusters=k, init='random', random_state=111)
    kmeans.fit(train_new_df)
    wcssd.append(kmeans.inertia_)
plt.plot(nr_clus, wcssd, marker='x')
plt.xlabel('Values of K')
plt.ylabel('Within Cluster Sum Squared Distance')
plt.title('Elbow Curve for Optimal K')
plt.show()

km_2cluster = KMeans(n_clusters=2, init='random', random_state=222)
km_2cluster_model = km_2cluster.fit_predict(train_new_df)

km_3cluster = kmclus(n_clusters=3, init='random', random_state=333)
km_3cluster_model = km_3cluster.fit_predict(train_new_df)

sscore_km_2cluster = sscore(train_new_df, km_2cluster_model);
print(f"Silhouette Score for 2 clusters: {sscore_km_2cluster}")
dbscore_km_2cluster = dbscore(train_new_df, km_2cluster_model);
print(f"Davies-Bouldin Index for 2 clusters: {dbscore_km_2cluster}\n")
sscore_km_3cluster = sscore(train_new_df, km_3cluster_model)
print(f"Silhouette Score for 3 clusters: {sscore_km_3cluster}")
dbscore_km_3cluster = dbscore(train_new_df, km_3cluster_model)
print(f"Davies-Bouldin Index for 3 clusters: {dbscore_km_3cluster}\n")

import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.datasets import make_blobs
import psutil

# Generate synthetic data
train_new_df=train_df[['Rating','Price','Minimum Installs','Category_code', 'Currency_code', 'Minimum Android_code', 'Content Rating_code', 'Free_code', 'Ad Supported_code', 'In App Purchases_code','Editors Choice_code']]

# Initialize K-Means object
kmeans = KMeans(n_clusters=2, random_state=42)

# Initialize DBSCAN object
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Measure memory before clustering
mem_before = psutil.Process().memory_info().rss / 1024**2  # Memory usage in MB

# K-Means clustering
start_time = time.time()
kmeans.fit(train_new_df)
kmeans_time = time.time() - start_time

# Measure memory after K-Means clustering
mem_after_kmeans = psutil.Process().memory_info().rss / 1024**2

# DBSCAN clustering
start_time = time.time()
dbscan.fit(train_new_df)
dbscan_time = time.time() - start_time

# Measure memory after DBSCAN clustering
mem_after_dbscan = psutil.Process().memory_info().rss / 1024**2

# Get cluster labels
kmeans_cluster_labels = kmeans.labels_
dbscan_cluster_labels = dbscan.labels_

# Calculate cluster sizes
kmeans_cluster_sizes = np.bincount(kmeans_cluster_labels[kmeans_cluster_labels >= 0])
dbscan_cluster_sizes = np.bincount(dbscan_cluster_labels[dbscan_cluster_labels >= 0])

# Composition of each cluster (for K-Means, we can use centroids)
kmeans_centroids = kmeans.cluster_centers_

# Print results
print("Size of Clusters (K-Means):", kmeans_cluster_sizes)
print("Size of Clusters (DBSCAN):", dbscan_cluster_sizes)

print("\nComparison on Memory and Time Taken:")
print("Time taken by K-Means: {:.4f} seconds".format(kmeans_time))
print("Memory used by K-Means: {:.2f} MB".format(mem_after_kmeans - mem_before))
print("\nTime taken by DBSCAN: {:.4f} seconds".format(dbscan_time))
print("Memory used by DBSCAN: {:.2f} MB".format(mem_after_dbscan - mem_before))

# Plot clusters for K-Means
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.scatter(train_new_df['Minimum Installs'], train_new_df['Rating'], c=kmeans_cluster_labels, cmap='viridis')
#plt.scatter(kmeans_centroids[:, 0], kmeans_centroids[:, 1], marker='x', s=100, color='red')
plt.title('K-Means Clustering')
plt.xlabel('Selling Price')
plt.ylabel('Kilometers Driven')
plt.colorbar(label='Cluster')

# Plot clusters for DBSCAN
plt.subplot(1, 2, 2)
plt.scatter(train_new_df['Minimum Installs'], train_new_df['Rating'], c=dbscan_cluster_labels, cmap='viridis')
plt.title('DBSCAN Clustering')
plt.xlabel('Selling Price')
plt.ylabel('Kilometers Driven')
plt.colorbar(label='Cluster')

plt.tight_layout()
plt.show()

print(train_new_df.columns)
plt.scatter(train_new_df['Price'], train_new_df['Rating'], c=kmeans_cluster_labels, cmap='viridis')
if 'Price' in train_new_df.columns:
    plt.scatter(train_new_df['Price'], train_new_df['Rating'], c=kmeans_cluster_labels, cmap='viridis')
else:
    # Handle the case where 'Rating Count' column is missing
    print("'Rating Count' column not found in the DataFrame")



"""**REPORT FOR MLM PROJECT 1 (K MEANS)**

**A. OBJECTIVE:**

The objective of this project is to preprocess and analyze Google Playstore Apps dataset, perform missing data treatment, encode categorical variables, and apply clustering techniques to identify patterns in the data. The final step is to present the findings in a comprehensive report.


**B. INSIGHTS:**
1. Missing Data Treatment:
- Categorical Data: Imputed missing values using the most frequent strategy (mode).
- Non-Categorical Data: Imputed missing values using the most frequent strategy (mode).

2. Data Encoding:
- Ordinal Encoding: Utilized the scikit-learn Ordinal Encoder to numerically encode categorical variables.

3. Data Transformation & Rescaling:
- Standardization: Applied Standard Scaler to rescale non-categorical data, ensuring uniformity in scale.

4. Clustering:
- K-Means Clustering: Determined the optimal number of clusters using the elbow method and evaluated performance with silhouette score and Davies-Bouldin index.


**C. ANALYSIS:**
- Explored K-Means clustering with two and three clusters.
- Utilized the elbow method to identify the optimal number of clusters.
- Evaluated clustering performance using silhouette score and Davies-Bouldin index.


**D. CONCLUSION:**
- K-Means clustering with two clusters yielded a higher silhouette score and lower Davies-Bouldin index, indicating better separation.
- DBSCAN clustering was also implemented to compare its performance.


**E. MANAGERIAL IMPLICATIONS:**
- Understanding app clusters can aid in targeted marketing strategies.
- Insights from clustering can influence app development and feature prioritization.


**F. RECOMMENDATIONS:**
- Continue exploring alternative clustering algorithms for comparison.
- Conduct further analysis on each cluster to identify specific characteristics.
- Explore additional features and their impact on clustering.
- Incorporate user reviews and engagement metrics for a more comprehensive analysis.
"""